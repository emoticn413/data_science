{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following task, you'll continue working with the [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/download) dataset from Kaggle. Before moving on to the tasks, you should load the dataset using Dask.\n",
    "\n",
    "Please submit your solutions to the following tasks as a link to your jupyter notebook on Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask-ml\n",
      "  Downloading dask_ml-1.7.0-py3-none-any.whl (141 kB)\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in b:\\program files\\phthon\\lib\\site-packages (from dask-ml) (2.30.0)\n",
      "Requirement already satisfied: scipy in b:\\program files\\phthon\\lib\\site-packages (from dask-ml) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in b:\\program files\\phthon\\lib\\site-packages (from dask-ml) (1.18.5)\n",
      "Collecting numba\n",
      "  Downloading numba-0.52.0-cp37-cp37m-win_amd64.whl (2.3 MB)\n",
      "Requirement already satisfied: packaging in b:\\program files\\phthon\\lib\\site-packages (from dask-ml) (20.4)\n",
      "Collecting dask-glm>=0.2.0\n",
      "  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting multipledispatch>=0.4.9\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: distributed>=2.4.0 in b:\\program files\\phthon\\lib\\site-packages (from dask-ml) (2.30.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in b:\\program files\\phthon\\lib\\site-packages (from dask-ml) (0.23.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in b:\\program files\\phthon\\lib\\site-packages (from dask-ml) (1.1.4)\n",
      "Requirement already satisfied: pyyaml in b:\\program files\\phthon\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (5.3.1)\n",
      "Requirement already satisfied: toolz>=0.8.2; extra == \"array\" in b:\\program files\\phthon\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.11.1)\n",
      "Requirement already satisfied: fsspec>=0.6.0; extra == \"dataframe\" in b:\\program files\\phthon\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.8.4)\n",
      "Requirement already satisfied: partd>=0.3.10; extra == \"dataframe\" in b:\\program files\\phthon\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.1.0)\n",
      "Requirement already satisfied: setuptools in b:\\program files\\phthon\\lib\\site-packages (from numba->dask-ml) (39.0.1)\n",
      "Collecting llvmlite<0.36,>=0.35.0\n",
      "  Downloading llvmlite-0.35.0-cp37-cp37m-win_amd64.whl (16.0 MB)\n",
      "Requirement already satisfied: six in b:\\program files\\phthon\\lib\\site-packages (from packaging->dask-ml) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in b:\\program files\\phthon\\lib\\site-packages (from packaging->dask-ml) (2.4.7)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in b:\\program files\\phthon\\lib\\site-packages (from dask-glm>=0.2.0->dask-ml) (1.5.0)\n",
      "Requirement already satisfied: psutil>=5.0 in b:\\program files\\phthon\\lib\\site-packages (from distributed>=2.4.0->dask-ml) (5.7.3)\n",
      "Requirement already satisfied: zict>=0.1.3 in b:\\program files\\phthon\\lib\\site-packages (from distributed>=2.4.0->dask-ml) (2.0.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in b:\\program files\\phthon\\lib\\site-packages (from distributed>=2.4.0->dask-ml) (2.3.0)\n",
      "Requirement already satisfied: click>=6.6 in b:\\program files\\phthon\\lib\\site-packages (from distributed>=2.4.0->dask-ml) (7.1.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in b:\\program files\\phthon\\lib\\site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in b:\\program files\\phthon\\lib\\site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\n",
      "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in b:\\program files\\phthon\\lib\\site-packages (from distributed>=2.4.0->dask-ml) (6.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in b:\\program files\\phthon\\lib\\site-packages (from scikit-learn>=0.23->dask-ml) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in b:\\program files\\phthon\\lib\\site-packages (from scikit-learn>=0.23->dask-ml) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in b:\\program files\\phthon\\lib\\site-packages (from pandas>=0.24.2->dask-ml) (2018.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in b:\\program files\\phthon\\lib\\site-packages (from pandas>=0.24.2->dask-ml) (2.7.3)\n",
      "Requirement already satisfied: locket in b:\\program files\\phthon\\lib\\site-packages (from partd>=0.3.10; extra == \"dataframe\"->dask[array,dataframe]>=2.4.0->dask-ml) (0.2.0)\n",
      "Requirement already satisfied: heapdict in b:\\program files\\phthon\\lib\\site-packages (from zict>=0.1.3->distributed>=2.4.0->dask-ml) (1.0.1)\n",
      "Installing collected packages: llvmlite, numba, multipledispatch, dask-glm, dask-ml\n",
      "Successfully installed dask-glm-0.2.0 dask-ml-1.7.0 llvmlite-0.35.0 multipledispatch-0.6.0 numba-0.52.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the 'b:\\program files\\phthon\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install dask-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "from dask.distributed import Client, progress\n",
    "from dask_ml.model_selection import train_test_split\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:54351</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:54350/status' target='_blank'>http://127.0.0.1:54350/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>8.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:54351' processes=4 threads=8, memory=8.00 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the data into Dask dataframe\n",
    "df = dd.read_csv('archive/creditcard.csv', dtype={'Time': 'float64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. In this task, you'll train several machine learning models from scikit-learn using Dask as the backend of joblib. This time, you need to use all the variables except `Class` as your feature set. `Class` variable will be your target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=3\n",
       "    int64\n",
       "      ...\n",
       "      ...\n",
       "      ...\n",
       "Name: Class, dtype: int64\n",
       "Dask Name: split, 3 tasks"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is our feature set\n",
    "X = df.drop(\"Class\", axis=1)\n",
    "\n",
    "# This is our target variable\n",
    "Y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Since our data can fit into memory\n",
    "# we persist them to the RAM.\n",
    "X_train.persist()\n",
    "X_test.persist()\n",
    "y_train.persist()\n",
    "y_test.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression training score is:  0.8603763122213139\n",
      "Logistic regression test score is:  0.8519312271238683\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    lr.fit(X_train.compute(), y_train.compute())\n",
    "\n",
    "    \n",
    "preds_train = lr.predict(X_train.values.compute())\n",
    "preds_test = lr.predict(X_test.values.compute())\n",
    "\n",
    "print(\"Logistic regression training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
    "print(\"Logistic regression test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting tree training score is:  0.8862927942278449\n",
      "Gradient boosting tree test score is:  0.864667368214243\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    gbc.fit(X_train.compute(), y_train.compute())\n",
    "    \n",
    "preds_train = gbc.predict(X_train.values.compute())\n",
    "preds_test = gbc.predict(X_test.values.compute())\n",
    "\n",
    "print(\"Gradient boosting tree training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
    "print(\"Gradient boosting tree test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest training score is:  0.9999648606505172\n",
      "Random forest test score is:  0.9878482896721802\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    rfc.fit(X_train.compute(), y_train.compute())\n",
    "    \n",
    "preds_train = rfc.predict(X_train.values.compute())\n",
    "preds_test = rfc.predict(X_test.values.compute())\n",
    "\n",
    "print(\"Random forest training score is: \", roc_auc_score(preds_train, y_train.values.compute()))\n",
    "print(\"Random forest test score is: \", roc_auc_score(preds_test, y_test.values.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare the results of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results, the best performing model is the random forest. It's performance in both training and test sets are higher than those of the logistic regression and gradient boosting tree. However, we should note that we run these models using their default parameters. Ideally, one should do hyperparameter tuning by doing something like grid search or random search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
